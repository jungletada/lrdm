base_config:
- config/logging.yaml
- config/wandb.yaml
- config/dataset_depth/dataset_train_weather.yaml

augmentation:
  lr_flip_p: 0.5

dataloader:
  num_workers: 8
  effective_batch_size: 30
  max_train_batch_size: 30
  seed: 3047  # to ensure continuity when resuming from checkpoint

model:
  dim: 24
  depths: [2, 4, 4, 2]
  num_heads: [4, 4, 4, 4]

trainer:
  name: LatentTrainer
  training_noise_scheduler:
    pretrained_path: checkpoint/restore
  init_seed: 3047  # use null to train w/o seeding
  save_period: 2000
  backup_period: 2000
  charbonnier_eps: 1e-6
  lpips_net: vgg
  warmup_iterations: 1000
  ramp_iterations: 1000

max_epoch: 10000  # a large enough number
max_iter: 40000   # usually converges at around 80k

optimizer:
  name: AdamW

lr: 1e-03
lr_scheduler:
  name: IterExponential
  kwargs:
    total_iter: 40000
    final_ratio: 0.005
    warmup_steps: 100
