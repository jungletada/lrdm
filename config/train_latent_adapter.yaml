base_config:
- config/logging.yaml
- config/wandb.yaml
- config/dataset_depth/dataset_train_weather.yaml

augmentation:
  lr_flip_p: 0.5

dataloader:
  num_workers: 8
  effective_batch_size: 32
  max_train_batch_size: 4
  seed: 3047  # to ensure continuity when resuming from checkpoint

model:
  dim: 64
  depths: [6, 4, 4, 6]
  num_heads: [4, 4, 4, 4]

trainer:
  name: LatentTrainer
  training_noise_scheduler:
    pretrained_path: checkpoint/restore
  init_seed: 3047  # use null to train w/o seeding
  save_period: 2000
  backup_period: 2000

max_epoch: 10000  # a large enough number
max_iter: 2000   # usually converges at around 80k

optimizer:
  name: AdamW

lr: 1e-03
lr_scheduler:
  name: IterExponential
  kwargs:
    total_iter: 2000
    final_ratio: 0.005
    warmup_steps: 10
