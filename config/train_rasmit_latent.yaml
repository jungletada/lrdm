base_config:
- config/logging.yaml
- config/wandb.yaml
- config/dataset_depth/dataset_train_weather.yaml

augmentation:
  lr_flip_p: 0.5

dataloader:
  num_workers: 8
  effective_batch_size: 64
  max_train_batch_size: 32
  seed: 3407  # to ensure continuity when resuming from checkpoint

model:
  dim: 64
  depths: [6, 4, 4, 6]
  num_heads: [4, 4, 4, 4]

trainer:
  name: RAMiTLatentTrainer
  training_noise_scheduler:
    pretrained_path: checkpoint/ramit_restore
  init_seed: 3407  # use null to train w/o seeding
  save_period: 2000
  backup_period: 2000

max_epoch: 10000  # a large enough number
max_iter: 80000   # usually converges at around 80k

optimizer:
  name: AdamW

lr: 6e-04
lr_scheduler:
  name: IterExponential
  kwargs:
    total_iter: 80000
    final_ratio: 0.01
    warmup_steps: 100
