base_config:
- config/logging.yaml
- config/wandb.yaml
- config/dataset_depth/dataset_train_weather.yaml

augmentation:
  lr_flip_p: 0.5

dataloader:
  num_workers: 8
  effective_batch_size: 64
  max_train_batch_size: 32
  seed: 2025  # to ensure continuity when resuming from checkpoint

trainer:
  name: RAMiTLatentTrainer
  training_noise_scheduler:
    pretrained_path: checkpoint/ramit_restore
  init_seed: 2025  # use null to train w/o seeding
  save_period: 500
  backup_period: 500

max_epoch: 10000  # a large enough number
max_iter: 20000   # usually converges at around 20k

optimizer:
  name: AdamW

loss:
  name: mse_loss
  kwargs:
    reduction: mean

lr: 3.0e-04
lr_scheduler:
  name: IterExponential
  kwargs:
    total_iter: 20000
    final_ratio: 0.01
    warmup_steps: 100
