base_config:
- config/logging.yaml
- config/wandb.yaml
- config/dataset_depth/dataset_train_weather.yaml

augmentation:
  lr_flip_p: 0.5

dataloader:
  num_workers: 8
  effective_batch_size: 64
  max_train_batch_size: 64
  seed: 2025  # to ensure continuity when resuming from checkpoint

trainer:
  name: RAMiTLatentTrainer
  training_noise_scheduler:
    pretrained_path: checkpoint/ramit_restore
  init_seed: 3407  # use null to train w/o seeding
  save_period: 2000
  backup_period: 2000

max_epoch: 10000  # a large enough number
max_iter: 80000   # usually converges at around 80k

optimizer:
  name: AdamW

loss:
  name: mse_loss
  kwargs:
    reduction: mean

lr: 1e-03
lr_scheduler:
  name: IterExponential
  kwargs:
    total_iter: 80000
    final_ratio: 0.005
    warmup_steps: 100
